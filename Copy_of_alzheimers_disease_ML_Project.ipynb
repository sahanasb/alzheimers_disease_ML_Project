{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanasb/alzheimers_disease_ML_Project/blob/main/Copy_of_alzheimers_disease_ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project: Alzheimer's Disease Prediction\n",
        "\n",
        "Objective: The goal of this project is to analyze a dataset of patient information to predict the likelihood of an Alzheimer's diagnosis. We will walk through the entire machine learning pipeline, from cleaning the data to evaluating multiple predictive models.[1-2]"
      ],
      "metadata": {
        "id": "3ox8tUpQQTs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workflow:\n",
        "\n",
        "- **Load and Inspect Data:** Check for missing values, duplicates, and incorrect data types.  \n",
        "- **Exploratory Data Analysis (EDA):** Visualize the data to find patterns and insights.  \n",
        "- **Data Preprocessing:** Clean the data and scale the features for modeling.  \n",
        "- **Model Training:** Train four different classification models.  \n",
        "- **Model Evaluation:** Compare the models using accuracy and confusion matrices.  \n",
        "- **Feature Importance:** Identify the most influential factors for prediction.  \n"
      ],
      "metadata": {
        "id": "OXgF5_9BQPWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwxjkWxXCfl8"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Loading and Inspecting the Data\n",
        "\n",
        "First, we need to load our dataset from the CSV file. After loading, we will perform several **health checks** to understand the quality of our data.[3]\n",
        "\n",
        "**Checks to be Performed:**\n",
        "\n",
        "- **`.head()`** → Look at the first few rows.  \n",
        "- **`.info()`** → Check data types and non-null values.  \n",
        "- **`.isnull().sum()`** → Count missing values in each column.  \n",
        "- **`.duplicated().sum()`** → Count any duplicate rows.  \n",
        "- **`.describe()`** → Get a statistical summary to spot outliers.  \n"
      ],
      "metadata": {
        "id": "unBEaMI_WG7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CORRECTED DATA PREPARATION ---\n",
        "\n",
        "# STEP 1: Load the Data\n",
        "# Always start by getting your data into the program.\n",
        "print(\"--- 1. Loading Data ---\")\n",
        "df = pd.read_csv('/content/sample_data/alzheimers_disease_data.csv')\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- 2. Checking Data Types ---\")\n",
        "# This gives a summary of the dataframe, including the data type of each column.\n",
        "df.info()\n",
        "\n",
        "print(\"--- 3. Checking for Missing Values ---\")\n",
        "# This counts the number of empty (NaN) cells in each column.\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "print(\"\\n--- 4. Checking for Duplicate Rows ---\")\n",
        "# This counts the total number of rows that are exact duplicates of another row.\n",
        "duplicate_rows = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows found: {duplicate_rows}\")\n",
        "\n",
        "print(\"\\n--- 5. Getting a Statistical Overview ---\")\n",
        "# This provides stats like mean, min, max, and standard deviation for each numeric column.\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "9S73Z5vSLTaA",
        "outputId": "a84d606a-af0e-4321-daeb-b7539343e4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Loading Data ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/alzheimers_disease_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-76797738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Always start by getting your data into the program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- 1. Loading Data ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/alzheimers_disease_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset loaded successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/alzheimers_disease_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Explore and Visualize the Data (EDA) ---\n",
        "print(\"\\n--- Generating Visualizations ---\")\n",
        "plt.figure(figsize=(18, 12))"
      ],
      "metadata": {
        "id": "qkoYJrEeDFWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 1: Age Distribution\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.histplot(df['Age'], kde=True, bins=30, color='skyblue')\n",
        "plt.title('Age Distribution of Patients')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')"
      ],
      "metadata": {
        "id": "seGugDAOE49A",
        "outputId": "2dd637fb-5c19-4fd3-a060-df53b9b88e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2336398260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot 1: Age Distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skyblue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Age Distribution of Patients'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 2: Diagnosis Distribution (Alzheimer's vs. No Alzheimer's)\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.countplot(x='Diagnosis', data=df, palette='pastel')\n",
        "plt.title('Diagnosis Distribution')\n",
        "plt.xlabel('Diagnosis (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Count')"
      ],
      "metadata": {
        "id": "sAt8rMovE45U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 3: Correlation Heatmap\n",
        "plt.figure(figsize=(15, 10)) #fig size\n",
        "# We select a subset of important columns for a clearer heatmap\n",
        "correlation_cols = ['Age', 'BMI', 'PhysicalActivity', 'DietQuality', 'SleepQuality', 'SystolicBP', 'CholesterolTotal', 'MMSE', 'Diagnosis'] #important feature\n",
        "correlation_matrix = df[correlation_cols].corr() #positive corelation (if one increases other too), neg vice versa\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\") #blue = neg corelation, red = positive\n",
        "plt.title('Correlation Heatmap of Key Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NE6VT2WGE4wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Cleaning and Preprocessing\n",
        "\n",
        "Now that we understand our data, we need to prepare it for modeling.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "- **Drop Unnecessary Columns:** Remove columns like `PatientID` and `DoctorInCharge` that are just identifiers and don't provide predictive information.  \n",
        "- **Handle Missing Values:** Based on our inspection, if there were any missing values, we will fill them. For simplicity, we'll use the **median** of each column.  \n",
        "- **Separate Features and Target:** Split the data into **X** (the features we use to make predictions) and **y** (the target we want to predict, which is `Diagnosis`).  \n",
        "- **Split the Data:** Divide the cleaned dataset into a **training set** (to teach the model) and a **testing set** (to evaluate it). We’ll use `stratify=y` to ensure both sets have a similar proportion of diagnoses.  [1]\n"
      ],
      "metadata": {
        "id": "f1o55fXaXMTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# STEP 3: Clean the Data\n",
        "# Perform all your cleaning steps on the complete dataset at once.\n",
        "print(\"\\n--- 2. Cleaning Data ---\")\n",
        "\n",
        "# We will remove columns that are not useful for making a prediction.\n",
        "# 'PatientID' and 'DoctorInCharge' are just labels and don't tell us anything about the patient's health.\n",
        "# 'axis=1' tells pandas we are dropping columns, not rows.\n",
        "df = df.drop(['PatientID', 'DoctorInCharge'], axis=1)\n",
        "print(\"Removed 'PatientID' and 'DoctorInCharge' columns.\")"
      ],
      "metadata": {
        "id": "VrxNwmk2RUct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Separate Features (X) and Target (y)\n",
        "# With a fully cleaned dataframe, now we can define our features and target.\n",
        "print(\"\\n--- 3. Separating Features and Target ---\")\n",
        "X = df.drop('Diagnosis', axis=1)\n",
        "y = df['Diagnosis']\n",
        "print(\"Features (X) and target (y) have been separated.\")\n",
        "\n",
        "\n",
        "# STEP 4: Split into Training and Testing Sets\n",
        "# This is the final step in preparation. We split our clean data for the model,\n",
        "# using stratification for a representative test set.\n",
        "print(\"\\n--- 4. Splitting Data for Training and Testing ---\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(\"Data successfully split into training (80%) and testing (20%) sets.\")\n",
        "\n",
        "# Your data is now perfectly prepared for the next stages: Scaling and Model Training!\n",
        "print(f\"\\nShape of the training features (X_train): {X_train.shape}\")\n",
        "print(f\"Shape of the testing features (X_test): {X_test.shape}\")"
      ],
      "metadata": {
        "id": "B5LSVmLmLN3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feature Scaling\n",
        "\n",
        "We will use **StandardScaler** to transform our data so it has a mean of **0** and a standard deviation of **1**.\n",
        "\n",
        "**Important:**  \n",
        "We fit the scaler **only on the training data**, and then use that same scaler to transform both the training and testing data.  \n",
        "This prevents **data leakage**.  \n"
      ],
      "metadata": {
        "id": "eyYX0St0Xnau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features. This standardizes the data (mean=0, variance=1),\n",
        "# which helps many models perform better.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "print(\"\\nData is split and scaled. Ready for modeling.\")"
      ],
      "metadata": {
        "id": "TDzGSBW4DFTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Training and Evaluation\n",
        "\n",
        "We have train **four different classification models** and evaluate their performance.  \n",
        "\n",
        "To measure how well each model performed, we will look at:  \n",
        "- **Accuracy Score**  \n",
        "- **Confusion Matrix**  \n",
        "\n",
        "Finally, we have **plot all confusion matrices together** for an easy side-by-side comparison.  \n",
        "\n",
        "**Models to be Trained:**\n",
        "- **Logistic Regression**  \n",
        "- **Decision Tree**  \n",
        "- **Random Forest**  \n",
        "- **k-Nearest Neighbors (k-NN)**  [4]\n"
      ],
      "metadata": {
        "id": "3N2ww64KX02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 4. Build and Evaluate Models ---\n",
        "\n",
        "# a) Linear Regression (for educational purposes)\n",
        "print(\"\\n--- Linear Regression ---\")\n",
        "# Note: Linear Regression predicts a continuous value (e.g., a price), not a class (Yes/No).\n",
        "# It's not suitable here, but we include it to show the difference.\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "y_pred_lin = lin_reg.predict(X_test)\n",
        "# We can round the predictions to 0 or 1 to calculate an \"accuracy\"\n",
        "y_pred_lin_class = np.round(y_pred_lin)\n",
        "accuracy_lin = accuracy_score(y_test, y_pred_lin_class)\n",
        "print(f\"Accuracy (after rounding): {accuracy_lin:.4f}\")\n",
        "print(\"As you can see, the accuracy is low because this is the wrong type of model for this problem.\")\n"
      ],
      "metadata": {
        "id": "19TsiD5_DFQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Logistic Regression\n",
        "print(\"\\n--- Logistic Regression ---\")\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
        "print(f\"Accuracy: {accuracy_log:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log))\n",
        "\n",
        "# Plotting the Confusion Matrix for Logistic Regression **\n",
        "cm_log = confusion_matrix(y_test, y_pred_log)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OYD8swjlEK8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Decision Tree\n",
        "print(\"\\n--- Decision Tree ---\")\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Accuracy: {accuracy_dt:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
        "\n",
        "# Plotting the Confusion Matrix for Decision Tree **\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges', cbar=False)\n",
        "plt.title('Confusion Matrix - Decision Tree')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C1WrOuxzEK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Random Forest\n",
        "print(\"\\n--- Random Forest ---\")\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Plotting the Confusion Matrix for Random Forest **\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "g2zzQVI2EKzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# e) k-Nearest Neighbors (k-NN)\n",
        "print(\"\\n--- k-Nearest Neighbors (k-NN) ---\")\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5) # We'll use 5 neighbors\n",
        "knn_clf.fit(X_train, y_train)\n",
        "y_pred_knn = knn_clf.predict(X_test)\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(f\"Accuracy: {accuracy_knn:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# Plotting the Confusion Matrix for k-NN\n",
        "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Purples', cbar=False)\n",
        "plt.title('Confusion Matrix - k-NN')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yCZ0b_DvEKr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Compare Model Performance ---\n",
        "print(\"\\n--- Comparing All Models ---\")\n",
        "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'k-NN']\n",
        "accuracies = [accuracy_log, accuracy_dt, accuracy_rf, accuracy_knn]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=accuracies, y=models, palette='viridis')\n",
        "plt.xlabel('Accuracy Score')\n",
        "plt.ylabel('Model')\n",
        "plt.title('Model Comparison by Accuracy')\n",
        "plt.xlim(0, 1.0)\n",
        "\n",
        "# Add accuracy values on the bars\n",
        "for index, value in enumerate(accuracies):\n",
        "    plt.text(value, index, f'{value:.2f}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iW4yr545DFNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Feature Importance\n",
        "\n",
        "Now that we've trained our models, let's identify which features were most **influential**.  \n",
        "\n",
        "We will use the **Random Forest** model for this, since it has a built-in feature for measuring importance.  \n",
        "\n",
        " A **higher importance score** means the feature contributed more to making correct predictions.  \n"
      ],
      "metadata": {
        "id": "hY36GUfj4Nk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6: FIND AND VISUALIZE THE MOST IMPORTANT FEATURES ---\n",
        "\n",
        "print(\"\\n--- 7. Finding the Most Important Features ---\")\n",
        "# Our trained Random Forest model has a property called 'feature_importances_'.\n",
        "# This gives us a score for how important each feature was in making predictions.\n",
        "\n",
        "# Get the importance scores from the trained model\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "# Get the names of the features\n",
        "feature_names = X.columns\n",
        "\n",
        "# Create a pandas Series to make it easier to view and sort the importances\n",
        "feature_importance_series = pd.Series(importances, index=feature_names)\n",
        "\n",
        "# Sort the features by importance in descending order\n",
        "sorted_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "# Print the top 10 most important features\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(sorted_importances.head(10))\n",
        "\n",
        "\n",
        "# --- Create a plot to visualize the feature importances ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=sorted_importances.head(10).values, y=sorted_importances.head(10).index, palette='viridis')\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Top 10 Feature Importances for Alzheimer's Prediction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2i7pBfIGSMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Assuming X_train_scaled, X_test_scaled, y_train, and y_test are already created\n",
        "# and all models like log_reg, dt_clf, rf_clf, knn_clf have been trained)\n",
        "\n",
        "# Let's get the predictions from each model again, just to be sure\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "y_pred_knn = knn_clf.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix for each model\n",
        "cm_log = confusion_matrix(y_test, y_pred_log)\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "\n",
        "# --- Plotting All Matrices Together ---\n",
        "\n",
        "# Create a figure and a set of subplots in a 2x2 grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle('Comparison of Confusion Matrices', fontsize=16)\n",
        "\n",
        "# Plot Logistic Regression Confusion Matrix\n",
        "sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0], cbar=False)\n",
        "axes[0, 0].set_title('Logistic Regression')\n",
        "axes[0, 0].set_xlabel('Predicted')\n",
        "axes[0, 0].set_ylabel('True')\n",
        "\n",
        "\n",
        "# Plot Decision Tree Confusion Matrix\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges', ax=axes[0, 1], cbar=False)\n",
        "axes[0, 1].set_title('Decision Tree')\n",
        "axes[0, 1].set_xlabel('Predicted')\n",
        "axes[0, 1].set_ylabel('True')\n",
        "\n",
        "\n",
        "# Plot Random Forest Confusion Matrix\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1, 0], cbar=False)\n",
        "axes[1, 0].set_title('Random Forest')\n",
        "axes[1, 0].set_xlabel('Predicted')\n",
        "axes[1, 0].set_ylabel('True')\n",
        "\n",
        "\n",
        "# Plot k-NN Confusion Matrix\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Purples', ax=axes[1, 1], cbar=False)\n",
        "axes[1, 1].set_title('k-Nearest Neighbors (k-NN)')\n",
        "axes[1, 1].set_xlabel('Predicted')\n",
        "axes[1, 1].set_ylabel('True')\n",
        "\n",
        "\n",
        "# Adjust layout to prevent titles from overlapping\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8FqpFpkWJzvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "In this project, we successfully built and evaluated several machine learning models to predict **Alzheimer's disease**.  \n",
        "\n",
        "**Summary of Findings:**  \n",
        "- The **Random Forest** model achieved the highest accuracy of 94%\n",
        "- The **confusion matrices** show that all models were effective at correctly identifying patients without Alzheimer's.  \n",
        "- The most important features for making a prediction were **MMSE, Age, and FunctionalAssessment**, indicating their strong correlation with the diagnosis.  \n",
        "\n"
      ],
      "metadata": {
        "id": "r46M15lt4cW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference  \n",
        "[1] C. Jose, Preety, and Vidushi. *Comparative Analysis of Machine Learning Algorithms for Predicting Alzheimer's Disease.*  \n",
        "2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS), Gobichettipalayam, India, 2024, pp. 1020–1022.  \n",
        "[https://doi.org/10.1109/ICUIS64676.2024.10867186](https://doi.org/10.1109/ICUIS64676.2024.10867186)\n",
        "\n",
        "[2] J. Neelaveni and M. S. G. Devasana.  \n",
        "*Alzheimer Disease Prediction using Machine Learning Algorithms.*  \n",
        "2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS), Coimbatore, India, 2020, pp. 101–104.  \n",
        "[https://doi.org/10.1109/ICACCS48705.2020.9074248](https://doi.org/10.1109/ICACCS48705.2020.9074248)  \n",
        "\n",
        "\n",
        "[3] A. D. Arya, S. Singh Verma, P. Chakarabarti, and R. Bishnoi.  \n",
        "*Prediction of Alzheimer's Disease – A Machine Learning Perspective with Ensemble Learning.*  \n",
        "2023 6th International Conference on Contemporary Computing and Informatics (IC3I), Gautam Buddha Nagar, India, 2023, pp. 2308–2313.  \n",
        "[https://doi.org/10.1109/IC3I59117.2023.10397683](https://doi.org/10.1109/IC3I59117.2023.10397683)\n",
        "\n",
        "[4] V. R, S. U, T. Shetty, and N. Guruprasad.  \n",
        "*Comprehensive Methodologies for Alzheimer’s Disease Prediction: A Machine Learning Approach.*  \n",
        "2024 International Conference on IoT, Communication and Automation Technology (ICICAT), Gorakhpur, India, 2024, pp. 31–35.  \n",
        "[https://doi.org/10.1109/ICICAT62666.2024.10923447](https://doi.org/10.1109/ICICAT62666.2024.10923447)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyt9uFlWYOHE"
      }
    }
  ]
}